{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMKRXcRJceHU44OfQht235m"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToseGfKcc-I4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#description : self learning chatbot\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0p4krExeCON",
        "colab_type": "code",
        "outputId": "8b673195-efae-4231-ae64-873353ddf1ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsWZoKp0e7Ku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzBZih9uecGl",
        "colab_type": "code",
        "outputId": "9c2ea417-00a2-4d49-f4f1-1f62005caa60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        }
      },
      "source": [
        "pip install newspaper3k"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 60\n",
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Collecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 19.8MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Collecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/0e/9ab599d6e78f0340bb1d1e28ddeacb38c8bb7f91a1b0eae9a24e9603782f/tldextract-2.2.2-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.2MB/s \n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Collecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 37.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.3->newspaper3k) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (46.0.0)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/23/9c/6e63c23c39e53d3df41c77a3d05a49a42c4e1383a6d2a5e3233161b89dbf/requests_file-1.4.3-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: tinysegmenter, jieba3k, feedfinder2, feedparser\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13539 sha256=c0a31bfc21af46fe8d1a33b0c713c2c4830481c4348f77e305d5e36033af55d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398406 sha256=9022b05337f7c17e48c9984af34c7054d5bc541befb148fa1fc5c692e1ad98c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3357 sha256=76b090199f7245eb67c4c306f61486990de734140fb1b47c2f283cdb81f2062b\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedparser: filename=feedparser-5.2.1-cp36-none-any.whl size=44940 sha256=f76dc8521eec2b8f9d8ea9252880ee83ac1936ef379ca6d7d73c7e5c700d57f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
            "Successfully built tinysegmenter jieba3k feedfinder2 feedparser\n",
            "Installing collected packages: tinysegmenter, jieba3k, cssselect, requests-file, tldextract, feedfinder2, feedparser, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.4.3 tinysegmenter-0.3 tldextract-2.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAdiKfOje8u7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from newspaper import Article\n",
        "import random\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "import numpy as np\n",
        "import warnings\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knUSvPCwfjZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ignore any warning messages \n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z8umNqBfygI",
        "colab_type": "code",
        "outputId": "f7e4cfbb-e7e1-416e-9f7e-95e576a505bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#download the packages from nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet',quiet=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrLDX-ORgF09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the article URL\n",
        "article=Article('https://www.mayoclinic.org/diseases-conditions/chronic-kidney-disease/symptoms-causes/syc-20354521')\n",
        "article.download()\n",
        "article.parse()\n",
        "article.nlp()\n",
        "corpus = article.text\n",
        "\n",
        "#Print the corpus/text\n",
        "print(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97UGEVHJgrwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenisation \n",
        "#Tokenization\n",
        "text = corpus\n",
        "sent_tokens = nltk.sent_tokenize(text) #Convert the text into a list of sentences\n",
        "\n",
        "#Print the list of sentences\n",
        "print(sent_tokens)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCz8uqXehf9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a dictionary (key:value) pair to remove punctuations\n",
        "remove_punct_dict = dict(  ( ord(punct),None) for punct in string.punctuation)\n",
        "\n",
        "#Print the punctuations\n",
        "print(string.punctuation)\n",
        "\n",
        "#Print the dictionary\n",
        "print(remove_punct_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijvM41cXiaaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a function to return a list of lemmatized lower case words after removing punctuations\n",
        "def LemNormalize(text):\n",
        "  return nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
        "\n",
        "#Print the tokenization text\n",
        "print(LemNormalize(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGIAFbPXjSt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Keyword Matching\n",
        "\n",
        "#Greeting Inputs\n",
        "GREETING_INPUTS = [\"hi\", \"hello\", \"hola\", \"greetings\", \"wassup\", \"hey\"]\n",
        "\n",
        "#Greeting responses back to the user\n",
        "GREETING_RESPONSES=[\"howdy\", \"hi\", \"hey\", \"what's good\", \"hello\", \"hey there\"]\n",
        "\n",
        "#Function to return a random greeting response to a users greeting\n",
        "def greeting(sentence):\n",
        "  #if the user's input is a greeting, then return a randomly chosen greeting response\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in GREETING_INPUTS:\n",
        "      return random.choice(GREETING_RESPONSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxmL6H6YjXxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def response(user_response):\n",
        "  \n",
        "\n",
        "  #The users response / query\n",
        "  #user_response = 'What is chronic kidney disease'\n",
        "\n",
        "  user_response = user_response.lower() #Make the response lower case\n",
        "\n",
        "  ###Print the users query/ response\n",
        "  #print(user_response)\n",
        "\n",
        "  #Set the chatbot response to an empty string\n",
        "  robo_response = ''\n",
        "\n",
        "  #Append the users response to the sentence list\n",
        "  sent_tokens.append(user_response)\n",
        "\n",
        "  ###Print the sentence list after appending the users response\n",
        "  #print(sent_tokens)\n",
        "\n",
        "  #Create a TfidfVectorizer Object\n",
        "  TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words='english')\n",
        "\n",
        "  #Convert the text to a matrix of TF-IDF features\n",
        "  tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "\n",
        "  ###Print the TFIDF features\n",
        "  #print(tfidf)\n",
        "\n",
        "  #Get the measure of similarity (similarity scores)\n",
        "  vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "\n",
        "  #Print the similarity scores\n",
        "  #print(vals)\n",
        "  #Get the index of the most similar text/sentence to the users response\n",
        "  idx = vals.argsort()[0][-2]\n",
        "\n",
        "  #Reduce the dimensionality of vals\n",
        "  flat = vals.flatten()\n",
        "\n",
        "  #sort the list in ascending order\n",
        "  flat.sort()\n",
        "\n",
        "  #Get the most similar score to the users response\n",
        "  score = flat[-2]\n",
        "\n",
        "  #Print the similarity score\n",
        "  #print(score)\n",
        "\n",
        "  #If the variable 'score' is 0 then their is no text similar to the users response\n",
        "  if(score == 0):\n",
        "    robo_response = robo_response+\"I apologize, I don't understand.\"\n",
        "  else:\n",
        "    robo_response = robo_response+sent_tokens[idx]\n",
        "  \n",
        "  #Print the chat bot response\n",
        "  #print(robo_response)\n",
        "  \n",
        "  #Remove the users response from the sentence tokens list\n",
        "  sent_tokens.remove(user_response)\n",
        "  \n",
        "  return robo_response"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPuw-XwAu_se",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flag = True\n",
        "print(\"DOCBot: I am Doctor Bot or DOCBot for short. I will answer your queries about Chronic Kidney Disease. If you want to exit, type Bye!\")\n",
        "while(flag == True):\n",
        "  user_response = input()\n",
        "  user_response = user_response.lower()\n",
        "  if(user_response != 'bye'):\n",
        "    if(user_response == 'thanks' or user_response =='thank you'):\n",
        "      flag=False\n",
        "      print(\"DOCBot: You are welcome !\")\n",
        "    else:\n",
        "      if(greeting(user_response) != None):\n",
        "        print(\"DOCBot: \"+greeting(user_response))\n",
        "      else:\n",
        "        print(\"DOCBot: \"+response(user_response))       \n",
        "  else:\n",
        "    flag = False\n",
        "    print(\"DOCBot: Chat with you later !\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}